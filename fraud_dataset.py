# -*- coding: utf-8 -*-
"""Fraud_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kKXim_tK_Lc49zWAmZ5bOPNk_rHa58UM
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

# 1 - Read dataset
# load the database
fraud_dataset = pd.read_csv("fraud_detection Dataset.csv")

# analyzing database and print some information about database
First_5_rows = fraud_dataset.head() # print first 5 rows of database
#print(First_5_rows)
# print database information
database_information = fraud_dataset.info()
#print(database_information)
# print database description
database_description = fraud_dataset.describe()
#print(database_description)

# 2 - Explore the data
# 1 - basic Exploration
print("First few rows of the dataset:")
print(fraud_dataset.head(10))

# Drop irrelevant columns (example: IDs)
irrelevant_columns = ['ID']  # put irrelevant columns to be dropped
for column in irrelevant_columns:
    if column in fraud_dataset.columns:
        fraud_dataset = fraud_dataset.drop(columns=irrelevant_columns)

# 2- Check Datatypes
print("\nColumn Datatypes:")
print(fraud_dataset.dtypes)  # Print the data types of each column

# Identify numerical and categorical columns
numerical_columns = fraud_dataset.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_columns = fraud_dataset.select_dtypes(include=['object']).columns.tolist()

print("\nNumerical Columns:", numerical_columns)
print("Categorical Columns:", categorical_columns)

# Convert categorical columns to 'category' datatype for better memory efficiency
for col in categorical_columns:
    fraud_dataset[col] = fraud_dataset[col].astype('category')
print("Print the data types of each column after Convert categorical columns to 'category' datatype")
print(fraud_dataset.dtypes)

# 3. Categorical Column Analysis
print("\nNumber of unique categories in each categorical column:")
for col in categorical_columns:
    print(f"{col}: {fraud_dataset[col].nunique()} unique categories")

# 4. Missing Values
print("\nMissing Values Analysis:")
missing_values = fraud_dataset.isnull().sum()  # Count of missing values in each column
missing_percentage = (missing_values / len(fraud_dataset)) * 100  # Percentage of missing values
print(missing_percentage)

# 3- handle missing values (our database doen't have missing values)
# 1. High Null Ratios: Drop columns with null-value percentage > 50%
high_null_cols = missing_percentage[missing_percentage > 50].index
print(f"Columns dropped due to high null ratio (>50%): {list(high_null_cols)}")
fraud_dataset = fraud_dataset.drop(columns=high_null_cols)

# 2. Categorical Columns: Fill missing values with the mode
for col in categorical_columns:
    mode_value = fraud_dataset[col].mode()  # Calculate the mode
    fraud_dataset[col].fillna(mode_value)


# 3. Numerical Columns: Visualize distribution and fill missing values
for col in numerical_columns:
    print(f"\nAnalyzing column: {col}")

    # Visualize the distribution
    plt.figure(figsize=(6, 4))
    fraud_dataset[col].hist(bins=30)
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.show()

    # Determine skewness
    skewness = fraud_dataset[col].skew()
    print(f"Skewness of {col}: {skewness}")

    if skewness > 0.5 or skewness < -0.5:  # Highly skewed
        median_value = fraud_dataset[col].median()
        print(f"Filling missing values in {col} with median: {median_value}")
        fraud_dataset[col].fillna(median_value)
    else:  # Symmetric distribution
        mean_value = fraud_dataset[col].mean()
        print(f"Filling missing values in {col} with mean: {mean_value}")
        fraud_dataset[col].fillna(mean_value)

# 4. Validate Null Handling: Recheck for missing values
missing_values_after = fraud_dataset.isnull().sum().sum()
if missing_values_after == 0:
    print("\nAll missing values have been handled successfully!")
else:
    print(f"\nRemaining missing values: {missing_values_after}")

# 1. Visualize Outliers: Use box plots to detect outliers in numerical columns
print("\nVisualizing Outliers:")
for col in numerical_columns:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x=fraud_dataset[col])
    plt.title(f"Box Plot of {col}")
    plt.show()

# 2. Capping Outliers: Define upper and lower bounds using IQR
for col in numerical_columns:
    Q1 = fraud_dataset[col].quantile(0.25)  # First quartile
    Q3 = fraud_dataset[col].quantile(0.75)  # Third quartile
    IQR = Q3 - Q1  # Interquartile range

    lower_bound = Q1 - 1.5 * IQR  # Lower whisker
    upper_bound = Q3 + 1.5 * IQR  # Upper whisker

    print(f"\nOutlier capping for column: {col}")
    print(f"Lower bound: {lower_bound}, Upper bound: {upper_bound}")

    # Replace values above upper whisker
    fraud_dataset[col] = fraud_dataset[col].apply(lambda x: upper_bound if x > upper_bound else x)
    # Replace values below lower whisker
    fraud_dataset[col] = fraud_dataset[col].apply(lambda x: lower_bound if x < lower_bound else x)

# 3. Categorical Outliers: Replace rare categories with the mode

for col in categorical_columns:
    # Calculate the frequency of each category
    category_counts = fraud_dataset[col].value_counts()
    print(f"\nCategory frequency in column '{col}':\n{category_counts}")

    # Identify rare categories (e.g., less than 5% of the dataset size)
    rare_threshold = len(fraud_dataset) * 0.05  # 5% threshold
    rare_categories = category_counts[category_counts < rare_threshold].index

    # Replace rare categories with the mode
    mode_value = fraud_dataset[col].mode()[0]
    fraud_dataset[col] = fraud_dataset[col].replace(rare_categories, mode_value)
    print(f"Replaced rare categories in '{col}' with mode: {mode_value}")

# Validate Outlier Treatment
print("\nFinal Dataset Summary:")
print(fraud_dataset.describe())

# 5. Check for Duplicates
print("\nNumber of duplicate rows in the dataset:", fraud_dataset.duplicated().sum())
fraud_dataset = fraud_dataset.drop_duplicates()
print("Duplicates removed!")

# 6 - Drop Low-Variance Columns
# Calculate standard deviation for each numerical column

std_deviation = fraud_dataset.select_dtypes(include=['int64', 'float64']).std()  # This computes the standard deviation for numerical columns
print("Standard Deviation for Numerical Columns:")
print(std_deviation)

# Define a threshold for low variance (e.g., std < 0.01)
low_variance_threshold = 0.01
low_variance_columns = std_deviation[std_deviation < low_variance_threshold].index

# Drop low-variance columns
print(f"Columns with low variance (std < {low_variance_threshold}): {list(low_variance_columns)}")
fraud_dataset = fraud_dataset.drop(columns=low_variance_columns)


# Drop irrelevant columns
irrelevant_columns = ['Credit_card_number', 'Expiry', 'Security_code']
fraud_dataset_cleaned = fraud_dataset.drop(columns=irrelevant_columns)
print(fraud_dataset_cleaned)

# Define the target column
target_column = 'Fraud'

# Split the dataset into features (X) and label (y)
X = fraud_dataset_cleaned.drop(columns=[target_column] , axis = 1)  # Features (independent variables)
y = fraud_dataset_cleaned[target_column]                # Label (dependent variable)

X = pd.get_dummies(X, columns=['Profession'])
# Display results
print("Features (X):")
print(X.head())

print("\nLabel (y):")
print(y.head())

# 1. Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the results
print("Training Features (X_train):")
print(X_train)
print("\nTesting Features (X_test):")
print(X_test)
print("\nTraining Target (y_train):")
print(y_train)
print("\nTesting Target (y_test):")
print(y_test)

# Train a Random Forest Classifier
rf_model = RandomForestClassifier(random_state=42)
rf_model = rf_model.fit(X_train, y_train)

# Train a Logistic Regression Model
lr_model = LogisticRegression(max_iter=500, solver = 'liblinear',random_state=42)
lr_model = lr_model.fit(X_train, y_train)

# Predictions from both models
rf_predictions = rf_model.predict(X_test)
lr_predictions = lr_model.predict(X_test)

# Evaluate Random Forest Model
print("Random Forest Evaluation:")

# Confusion Matrix for Random Forest
rf_cm = confusion_matrix(y_test, rf_predictions)
print("Confusion Matrix:\n", rf_cm)

# F1-Score and other classification metrics
rf_classification_report = classification_report(y_test, rf_predictions)
print("Classification Report:\n", rf_classification_report)

# Accuracy of Random Forest
rf_accuracy = accuracy_score(y_test, rf_predictions)
print("Accuracy:", rf_accuracy)


print("\n")

# Evaluate Logistic Regression Model
print("Logistic Regression Evaluation:")

# Confusion Matrix for Logistic Regression
lr_cm = confusion_matrix(y_test, lr_predictions)
print("Confusion Matrix:\n", lr_cm)

# F1-Score and other classification metrics
lr_classification_report = classification_report(y_test, lr_predictions)
print("Classification Report:\n", lr_classification_report)

# Accuracy of Logistic Regression
lr_accuracy = accuracy_score(y_test, lr_predictions)
print("Accuracy:", lr_accuracy)

lr_accuracy = lr_model.score(X_test, y_test)
print("Logistic Regression Accuracy:", lr_accuracy)